{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converts ARGO/PR/PF to a Parquet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_list(array):\n",
    "    \"\"\"Converts a numpy array into a Python list\"\"\"\n",
    "    if array.dtype != np.dtype(\"S1\"):\n",
    "        return [item for item in array[:, ]]\n",
    "    else:\n",
    "        return [item.tostring().decode() for item in array[:, ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    \"\"\"Load a NetCDF file and transform it into a pandas DataFrame\"\"\"\n",
    "    # Definitions of the dataframe schema\n",
    "    cols = dict(datetime=None,\n",
    "                config_mission_number=None,\n",
    "                cycle_number=None,\n",
    "                data_centre=None,\n",
    "                data_mode=None,\n",
    "                data_state_indicator=None,\n",
    "                data_type=None,\n",
    "                date_creation=None,\n",
    "                date_qc=None,\n",
    "                date_update=None,\n",
    "                dc_reference=None,\n",
    "                depth=None,\n",
    "                depth_adjusted=None,\n",
    "                depth_adjusted_error=None,\n",
    "                depth_adjusted_qc=None,\n",
    "                depth_qc=None,\n",
    "                direction=None,\n",
    "                firmware_version=None,\n",
    "                float_serial_no=None,\n",
    "                hdyn=None,\n",
    "                latitude=None,\n",
    "                longitude=None,\n",
    "                original_file_name=None,\n",
    "                pi_name=None,\n",
    "                platform_number=None,\n",
    "                platform_type=None,\n",
    "                position_qc=None,\n",
    "                positioning_system=None,\n",
    "                pres=None,\n",
    "                pres_adjusted=None,\n",
    "                pres_adjusted_error=None,\n",
    "                pres_adjusted_qc=None,\n",
    "                pres_qc=None,\n",
    "                project_name=None,\n",
    "                psal=None,\n",
    "                psal_adjusted=None,\n",
    "                psal_adjusted_error=None,\n",
    "                psal_adjusted_qc=None,\n",
    "                psal_qc=None,\n",
    "                station_parameters=None,\n",
    "                sla=None,\n",
    "                temp=None,\n",
    "                temp_adjusted=None,\n",
    "                temp_adjusted_error=None,\n",
    "                temp_adjusted_qc=None,\n",
    "                temp_qc=None,\n",
    "                vertical_sampling_scheme=None,\n",
    "                wmo_inst_type=None)\n",
    "\n",
    "    dtypes = dict(datetime=np.datetime64,\n",
    "                  config_mission_number=np.int32,\n",
    "                  cycle_number=np.int32,\n",
    "                  data_centre=str,\n",
    "                  data_mode=str,\n",
    "                  data_state_indicator=str,\n",
    "                  data_type=str,\n",
    "                  date_creation=np.datetime64,\n",
    "                  date_qc=str,\n",
    "                  date_update=np.datetime64,\n",
    "                  dc_reference=np.int64,\n",
    "                  depth=[np.float32],\n",
    "                  depth_adjusted=[np.float32],\n",
    "                  depth_adjusted_error=[np.float32],\n",
    "                  depth_adjusted_qc=[str],\n",
    "                  depth_qc=[str],\n",
    "                  direction=str,\n",
    "                  firmware_version=str,\n",
    "                  float_serial_no=str,\n",
    "                  hdyn=np.float32,\n",
    "                  latitude=np.float64,\n",
    "                  longitude=np.float64,\n",
    "                  original_file_name=str,\n",
    "                  pi_name=str,\n",
    "                  platform_number=str,\n",
    "                  platform_type=str,\n",
    "                  position_qc=str,\n",
    "                  positioning_system=str,\n",
    "                  pres=[np.float32],\n",
    "                  pres_adjusted=[np.float32],\n",
    "                  pres_adjusted_error=[np.float32],\n",
    "                  pres_adjusted_qc=[str],\n",
    "                  pres_qc=[str],\n",
    "                  project_name=str,\n",
    "                  psal=[np.float32],\n",
    "                  psal_adjusted=[np.float32],\n",
    "                  psal_adjusted_error=[np.float32],\n",
    "                  psal_adjusted_qc=[str],\n",
    "                  psal_qc=[str],\n",
    "                  sla=np.float32,\n",
    "                  station_parameters=[str],\n",
    "                  temp=[np.float32],\n",
    "                  temp_adjusted=[np.float32],\n",
    "                  temp_adjusted_error=[np.float32],\n",
    "                  temp_adjusted_qc=[str],\n",
    "                  temp_qc=[str],\n",
    "                  vertical_sampling_scheme=str,\n",
    "                  wmo_inst_type=str)\n",
    "\n",
    "    with netCDF4.Dataset(path, \"r\") as ds:\n",
    "        # Axis of this dataset\n",
    "        time, levels = len(ds.dimensions[\"N_PROF\"]), len(\n",
    "            ds.dimensions[\"N_LEVELS\"])\n",
    "\n",
    "        for name, item in ds.variables.items():\n",
    "            values = item[:]\n",
    "\n",
    "            # Axis : the axes must not contain undefined values\n",
    "            if name in [\"JULD\", \"LATITUDE\", \"LONGITUDE\"]:\n",
    "                if isinstance(values, np.ma.MaskedArray):\n",
    "                    if np.ma.is_masked(values) and name == \"JULD\":\n",
    "                        return None\n",
    "                    values = values.data\n",
    "                if len(values) != time:\n",
    "                    assert len(values) == 1\n",
    "                    values = np.full((time, ), values[0], dtype=values.dtype)\n",
    "                if name == 'JULD':\n",
    "                    cols[\"datetime\"] = pd.Series(\n",
    "                        netCDF4.num2date(values, item.units))\n",
    "                    cols[\"partition\"] = cols[\"datetime\"].apply(lambda x: x.date().replace(day=1))\n",
    "                else:\n",
    "                    cols[name.lower()] = values\n",
    "                continue\n",
    "\n",
    "            # Process numpy MaskedArray\n",
    "            if isinstance(\n",
    "                    values,\n",
    "                    np.ma.MaskedArray) and values.dtype != np.dtype(\"S1\"):\n",
    "                values[values.mask] = netCDF4.default_fillvals[\n",
    "                    values.dtype.\n",
    "                    str[1:]] if values.dtype.kind != 'f' else np.nan\n",
    "                values = values.data\n",
    "\n",
    "            # Transform the data type into a vector\n",
    "            # (new column of the DataFrame)\n",
    "            if name == \"DATA_TYPE\":\n",
    "                values = [values.tostring().decode()] * time\n",
    "            # Transforms the matrix of char into an array of strings\n",
    "            elif name == \"STATION_PARAMETERS\":\n",
    "                values = values.data\n",
    "                values = [[\n",
    "                    values[ix, jx, :].tostring().decode().strip()\n",
    "                    for jx in range(values.shape[1])\n",
    "                ] for ix in range(values.shape[0])]\n",
    "            # Transforms matrix into an array of Python lists\n",
    "            elif (item.dimensions == (\"N_PROF\", \"N_LEVELS\")) or (len(\n",
    "                    values.shape) == 2 and item.dimensions[0] == \"N_PROF\"):\n",
    "                values = array_to_list(values)\n",
    "            # Converts arrays of chars into string\n",
    "            elif values.dtype == np.dtype(\"S1\"):\n",
    "                string = values.tostring().decode()\n",
    "                values = list(string) if item.dimensions == (\n",
    "                    \"N_PROF\", ) else string\n",
    "            # Converts a scalar into a new column\n",
    "            elif item.dimensions[0] == \"N_LEVELS\" and levels == 1:\n",
    "                values = [values] * time\n",
    "            # Not handled\n",
    "            elif item.dimensions[0] == \"N_LEVELS\":\n",
    "                raise RuntimeError((path, name))\n",
    "            # Transforms column name into lower case\n",
    "            if name.lower() in cols:\n",
    "                cols[name.lower()] = values\n",
    "\n",
    "        # For all loaded data, the values are casted into the specified\n",
    "        # dataframe type.\n",
    "        for k, v in cols.items():\n",
    "            if v is None:\n",
    "                dtype = dtypes[k]\n",
    "                if isinstance(dtype, list):\n",
    "                    dtype = dtype[0]\n",
    "                    if dtype != str:\n",
    "                        cols[k] = [\n",
    "                            item for item in np.full(\n",
    "                                (time, levels), np.nan, dtype=dtype)[:, ]\n",
    "                        ]\n",
    "                    else:\n",
    "                        cols[k] = [' ' * levels for _ in range(time)]\n",
    "                elif dtype == np.dtype(\"float32\"):\n",
    "                    cols[k] = np.full((time,), np.nan, dtype=dtype)\n",
    "                elif dtype == np.datetime64:\n",
    "                    cols[k] = np.datetime64()\n",
    "                elif dtype == str:\n",
    "                    cols[k] = \"\"\n",
    "\n",
    "        df = pd.DataFrame(cols)\n",
    "        # Strip strings\n",
    "        for key in [\n",
    "                \"data_state_indicator\", \"data_type\", \"firmware_version\",\n",
    "                \"float_serial_no\", \"pi_name\", \"platform_number\",\n",
    "                \"platform_type\", \"positioning_system\",\n",
    "                \"vertical_sampling_scheme\", \"wmo_inst_type\"\n",
    "        ]:\n",
    "            df.loc[:, key] = df.loc[:, key].apply(lambda x: x.strip())\n",
    "        df[\"original_file_name\"] = os.path.basename(path)\n",
    "        # Transformation of some types contained in the columns. It's faster\n",
    "        # to do it here on the dataframe pandas.\n",
    "        df.loc[:, \"dc_reference\"] = df.loc[:, \"dc_reference\"].apply(lambda x:\n",
    "                                                                    int(x))\n",
    "        df.loc[:, \"date_creation\"] = df.loc[:, \"date_creation\"].apply(\n",
    "            lambda x: datetime.datetime.strptime(x, \"%Y%m%d%H%M%S\"))\n",
    "        df.loc[:, \"date_update\"] = df.loc[:, \"date_update\"].apply(\n",
    "            lambda x: datetime.datetime.strptime(x, \"%Y%m%d%H%M%S\"))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process one file to test the conversion\n",
    "load(os.path.join(dirname, \"CO_DMQCGL01_20000510_PR_PF.nc\")).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_db(dirname, df):\n",
    "    \"\"\"Function to write or dataset\"\"\"\n",
    "    # During the conversion, we defined a column labeled \"partition\" defining\n",
    "    # the date of our data. This column will be used to group our dataset by\n",
    "    # month. It's possible to do it differently, for example by days.\n",
    "    partition_keys = [df[\"partition\"]]\n",
    "    data_df = df.drop(\"partition\", axis='columns')\n",
    "    for key, subgroup in data_df.groupby(partition_keys):\n",
    "        outfile = None\n",
    "        subdir = os.path.join(dirname, f'year={key.year}',\n",
    "                              f'month={key.month}')\n",
    "        update = os.path.exists(subdir)\n",
    "        # Handles of dataframe update.\n",
    "        if update:\n",
    "            files = list(os.listdir(subdir))\n",
    "            if len(files) == 1:\n",
    "                outfile = os.path.join(subdir, files.pop())\n",
    "                subgroup = pd.concat([pd.read_parquet(outfile), subgroup])\n",
    "            elif len(files) == 0:\n",
    "                pass\n",
    "            else:\n",
    "                raise RuntimeError(files)\n",
    "        else:\n",
    "            os.makedirs(subdir)\n",
    "\n",
    "        if outfile is None:\n",
    "            outfile = f'{uuid.uuid4().hex}.parquet'\n",
    "        # TODO: lock file before write\n",
    "        subgroup.to_parquet(os.path.join(subdir, outfile),\n",
    "                            index=False,\n",
    "                            compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A minimalist solution to avoid reprocessing files twice.\n",
    "def write_buffer(dirname, buffer, files):\n",
    "    write_db(dirname, pd.concat(buffer))\n",
    "    for item in files:\n",
    "        with open(f\"{item}.done\", \"w\") as stream:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"argo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now start the conversion. We'll process the data in blocks to avoid\n",
    "# doing too many OIs. We make blocks of 64 files, it is the computer RAM that\n",
    "# acts as a limit here.\n",
    "blocs = []\n",
    "files = []\n",
    "\n",
    "def netcdf_2_parquet(dirname, blocs, files):\n",
    "    write_buffer(dirname, blocs, files)\n",
    "    blocs.clear()\n",
    "    files.clear()    \n",
    "\n",
    "for item in sorted(os.listdir(dirname)):\n",
    "    # Skip the file already processed\n",
    "    if 'PR_PF' not in item or item.endswith(\".done\"):\n",
    "        continue\n",
    "    path = os.path.join(dirname, item)\n",
    "    blocs.append(load(path))\n",
    "    files.append(path)\n",
    "    if len(blocs) > 64:\n",
    "        netcdf_2_parquet(root, blocs, files)\n",
    "if len(blocs):\n",
    "    netcdf_2_parquet(root, blocs, files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading our file\n",
    "import pyarrow.parquet as pq\n",
    "pq.read_table(root, filters=[('year', '==', '2000')]).to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
