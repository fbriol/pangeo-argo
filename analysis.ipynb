{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "\n",
    "gcsfs.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import datetime\n",
    "import logging\n",
    "import re\n",
    "import os\n",
    "import dask.dataframe\n",
    "import dask.distributed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyinterp.backends.xarray\n",
    "import pyinterp.geodetic\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration du cluster local Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_kubernetes\n",
    "\n",
    "cluster = dask_kubernetes.KubeCluster()\n",
    "cluster.adapt(minimum=1, maximum=10)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dask_dataframe(\n",
    "        dirname: str,\n",
    "        start: Optional[datetime.date] = None,\n",
    "        end: Optional[datetime.date] = None,\n",
    "        index: Optional[bool] = False,\n",
    ") -> dask.dataframe.DataFrame:\n",
    "    \"\"\"Select the data frame to process between two dates\"\"\"\n",
    "    if start is None:\n",
    "        start = datetime.date(1995, 1, 1)\n",
    "    if end is None:\n",
    "        end = datetime.date.today()\n",
    "    ddf = dask.dataframe.read_parquet(dirname,\n",
    "                                      engine=\"pyarrow\",\n",
    "                                      filters=[('year', '>=', start.year),\n",
    "                                               ('month', '>=', start.month),\n",
    "                                               ('year', '<=', end.year),\n",
    "                                               ('month', '<=', end.month)])\n",
    "    ddf = ddf[(ddf.datetime > start.isoformat())\n",
    "              & (ddf.datetime <= end.isoformat())]\n",
    "    if index:\n",
    "        ddf = ddf.set_index(\"datetime\")\n",
    "    return ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection géographique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _select_area(ddf: dask.dataframe.DataFrame, box: pyinterp.geodetic.Box2D):\n",
    "    \"\"\"Applies geographic selection to a DataFrame of a partition\"\"\"\n",
    "    return list(\n",
    "        box.covered_by(ddf.longitude.values, ddf.latitude.values).astype(bool))\n",
    "\n",
    "\n",
    "def select_area(ddf: dask.dataframe.DataFrame, box: pyinterp.geodetic.Box2D):\n",
    "    \"\"\"Applies geographic selection to a DataFrame\"\"\"\n",
    "    return ddf.map_partitions(_select_area, box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path the Parquet dataset\n",
    "path = \"gs://pangeo-cnes/argo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a small dataset (You can increase the size of data to read, but it\n",
    "# will take longer on our virtual machine)\n",
    "ddf = get_dask_dataframe(\n",
    "    path,\n",
    "    datetime.date(1990, 1, 1),\n",
    "    datetime.date(2019, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the data selection box.\n",
    "area = pyinterp.geodetic.Box2D(\n",
    "    pyinterp.geodetic.Point2D(-80, 7),\n",
    "    pyinterp.geodetic.Point2D(0,60))\n",
    "area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the query\n",
    "df = ddf[select_area(ddf, area)].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the result\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree(central_longitude=180))\n",
    "sc = ax.scatter(\n",
    "    df.longitude,\n",
    "    df.latitude,\n",
    "    1,\n",
    "    c=[item[0] for item in df.temp],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='jet')\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "fig.colorbar(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection par numéro de plateforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ddf[ddf.platform_number.isin(['2901216', '6900381', '5901026', '2902557'])]\n",
    "df = df[['datetime', 'longitude', 'latitude', 'temp']]\n",
    "df = df.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree(central_longitude=180))\n",
    "sc = ax.scatter(\n",
    "    df.longitude,\n",
    "    df.latitude,\n",
    "    1,\n",
    "    c=[item[0] for item in df.temp],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='jet')\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "fig.colorbar(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul d'une anomalie de pression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = get_dask_dataframe(\n",
    "    path,\n",
    "    datetime.date(1990, 1, 1),\n",
    "    datetime.date(2019, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pressure_anomalies(df):\n",
    "    \"\"\"Calculates pressure anomalies\"\"\"\n",
    "    return df.pres - df.pres_adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here only columns containing the longitude and latitude of the floats are\n",
    "# selected.\n",
    "df = ddf[['longitude', 'latitude']].compute()\n",
    "df['anomalies'] = ddf.map_partitions(\n",
    "    pressure_anomalies, meta=(None, 'f8')).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The average anomaly is calculated\n",
    "df['mean_anomalies'] = df['anomalies'].map(\n",
    "    lambda series: np.nan if np.all(np.isnan(series)) else np.nanmean(series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree(central_longitude=180))\n",
    "sc = ax.scatter(\n",
    "    df.longitude,\n",
    "    df.latitude,\n",
    "    1,\n",
    "    c=df.mean_anomalies,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='jet',\n",
    "    vmin=-1,\n",
    "    vmax=1)\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "fig.colorbar(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLA interpolation on Argo float positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridSeries:\n",
    "    \"\"\"Handles a series of grids stored in zarr format. This series is a\n",
    "    time series.\"\"\"\n",
    "    def __init__(self, ds):\n",
    "        self.ds = ds\n",
    "        self.series, self.dt = self._load_ts()\n",
    "        \n",
    "    @staticmethod\n",
    "    def _is_sorted(array):\n",
    "        indices = np.argsort(array)\n",
    "        return np.all(indices == np.arange(len(indices)))\n",
    "\n",
    "    def _load_ts(self):\n",
    "        \"\"\"Loading the time series into memory.\"\"\"\n",
    "        time = self.ds.time\n",
    "        assert self._is_sorted(time)\n",
    "\n",
    "        series = pd.Series(time)\n",
    "        frequency = set(np.diff(series.values.astype(\"datetime64[s]\")).astype(\"int64\"))\n",
    "        if len(frequency) != 1:\n",
    "            raise RuntimeError(\n",
    "                \"Time series does not have a constant step between two \"\n",
    "                f\"grids: {frequency} seconds\")\n",
    "        return series, datetime.timedelta(seconds=float(frequency.pop()))\n",
    "    \n",
    "    def load_dataset(self, varname, start, end):\n",
    "        \"\"\"Loading the time series into memory for the defined period.\n",
    "\n",
    "        Args:\n",
    "            varname (str): Name of the variable to be loaded into memory.\n",
    "            start (datetime.datetime): Date of the first map to be loaded.\n",
    "            end (datetime.datetime): Date of the last map to be loaded.\n",
    "\n",
    "        Return:\n",
    "            pyinterp.backends.xarray.Grid3D: The interpolator handling the\n",
    "            interpolation of the grid series.\n",
    "        \"\"\"\n",
    "        if start < self.series.min() or end > self.series.max():\n",
    "            raise IndexError(\n",
    "                f\"period [{start}, {end}] out of range [{self.series.min()}, \"\n",
    "                f\"{self.series.max()}]\")\n",
    "        first = start - self.dt\n",
    "        last = end + self.dt\n",
    "\n",
    "        selected = self.series[(self.series >= first) & (self.series < last)]\n",
    "        print(f\"fetch data from {selected.min()} to {selected.max()}\")\n",
    "        \n",
    "        data_array = ds[varname].isel(time=selected.index)\n",
    "        return pyinterp.backends.xarray.Grid3D(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(df, grid_series, varname):\n",
    "    \"\"\"Interpolate a variable 'varname' described by the time series\n",
    "    'grid_series' for the locations provided in the DataFrame 'df'\"\"\"\n",
    "    if not len(df):\n",
    "        return np.array([])\n",
    "    # The DataFrame must be ordered by the time axis\n",
    "    df = df.set_index(\"datetime\")\n",
    "    # The time axis is divided into monthly periods\n",
    "    period_start = df.groupby(df.index.to_period('M'))[\"sla\"].count().index\n",
    "    periods = []\n",
    "\n",
    "    end = None\n",
    "\n",
    "    # Calculates the period required to interpolate the data from the provided\n",
    "    # time series\n",
    "    for start, end in zip(period_start, period_start[1:]):\n",
    "        start = start.to_timestamp()\n",
    "        if start < grid_series.df.index[0]:\n",
    "            start = grid_series.df.index[0]\n",
    "        end = end.to_timestamp()\n",
    "        periods.append((start, end))\n",
    "    if end is None:\n",
    "        end = period_start[0].to_timestamp()\n",
    "    periods.append((end, df.index[-1] + datetime.timedelta(seconds=3600)))\n",
    "\n",
    "    # Finally, the data on the different periods identified are interpolated.\n",
    "    result = []\n",
    "    for start, end in periods:\n",
    "        interpolator = grid_series.load_dataset(varname, start, end)\n",
    "        mask = (df.index >= start) & (df.index < end)\n",
    "        selected = df.loc[mask, [\"longitude\", \"latitude\"]]\n",
    "        result.append(\n",
    "            interpolator.trivariate(dict(\n",
    "                longitude=selected[\"longitude\"].values,\n",
    "                latitude=selected[\"latitude\"].values,\n",
    "                time=selected.index.values),\n",
    "                                    interpolator=\"inverse_distance_weighting\",\n",
    "                                    num_threads=1))\n",
    "    return pd.Series(np.hstack(result), df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time series\n",
    "import intake\n",
    "cat = intake.Catalog(\"https://raw.githubusercontent.com/pangeo-data/pangeo-datastore\"\n",
    "                     \"/master/intake-catalogs/ocean.yaml\")\n",
    "ds = cat[\"sea_surface_height\"].to_dask()\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "ds = ds.drop(\"crs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_series = GridSeries(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data from dataset\n",
    "ddf = get_dask_dataframe(\n",
    "    path,\n",
    "    datetime.date(1990, 1, 1),\n",
    "    datetime.date(2019, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of SLA\n",
    "sla = ddf.map_partitions(interpolate, grid_series, 'sla', meta=('result', np.float64)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of a DataFrame containing the float positions and the\n",
    "# interpolated SLA.\n",
    "df = ddf[[\"datetime\", \"longitude\", \"latitude\"]].compute()\n",
    "df = df.join(sla, on=\"datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = df.datetime.min()\n",
    "last = df.datetime.max()\n",
    "size = (df.datetime - first) / (last-first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection=ccrs.PlateCarree(central_longitude=180))\n",
    "sc = ax.scatter(\n",
    "    df.longitude,\n",
    "    df.latitude,\n",
    "    s=size*100,\n",
    "    c=df.result,\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap='jet')\n",
    "ax.coastlines()\n",
    "ax.set_title(\"Time series of SLA \"\n",
    "             \"(larger points are closer to the last date)\")\n",
    "ax.add_feature(cfeature.LAND)\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.set_extent([80, 100, 13.5, 25], crs=ccrs.PlateCarree())\n",
    "fig.colorbar(sc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
